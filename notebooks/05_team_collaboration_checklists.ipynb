{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Team Collaboration Checklists\n",
    "\n",
    "Use these lists during planning, implementation, and review."
   ],
   "id": "ef2cd28a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_checklist = [\n",
    "    \"Correct behavior\",\n",
    "    \"Tests updated\",\n",
    "    \"Logging and errors are clear\",\n",
    "    \"No secrets in code or notebook outputs\",\n",
    "]\n",
    "review_checklist"
   ],
   "id": "d7ca670d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR checklist\n",
    "- [ ] Small scope\n",
    "- [ ] Linked issue\n",
    "- [ ] Repro steps included\n",
    "- [ ] At least one reviewer requested\n",
    "\n",
    "## Recall + transfer\n",
    "Explain from memory how you would review a pipeline change in 5 minutes, then adapt the same review flow to notebook-only work."
   ],
   "id": "6b9978c8"
  },
  {
   "cell_type": "markdown",
   "id": "c282ff32",
   "metadata": {},
   "source": [
    "## Practice Lab\n",
    "\n",
    "Simulate a fast PR triage decision based on scope, tests, and secrets checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triage_pr(changed_files: int, tests_passed: bool, secrets_detected: bool) -> str:\n",
    "    if secrets_detected:\n",
    "        return \"block\"\n",
    "    if not tests_passed:\n",
    "        return \"request-changes\"\n",
    "    if changed_files > 20:\n",
    "        return \"split-pr\"\n",
    "    return \"approve\"\n",
    "\n",
    "decisions = [\n",
    "    triage_pr(changed_files=8, tests_passed=True, secrets_detected=False),\n",
    "    triage_pr(changed_files=35, tests_passed=True, secrets_detected=False),\n",
    "    triage_pr(changed_files=5, tests_passed=False, secrets_detected=False),\n",
    "]\n",
    "decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0afa3",
   "metadata": {},
   "source": [
    "## Active Learning Practice\n",
    "\n",
    "1. Rate each PR scenario using the rubric.\n",
    "2. Compare outcomes with a teammate.\n",
    "3. Identify one missing review criterion and add it.\n",
    "4. Re-run scoring after updating the rubric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d763c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = {\"tests\": 3, \"docs\": 2, \"small_scope\": 2, \"security\": 3}\n",
    "\n",
    "def pr_score(*, tests: bool, docs: bool, small_scope: bool, security: bool) -> int:\n",
    "    return (\n",
    "        rubric[\"tests\"] * int(tests)\n",
    "        + rubric[\"docs\"] * int(docs)\n",
    "        + rubric[\"small_scope\"] * int(small_scope)\n",
    "        + rubric[\"security\"] * int(security)\n",
    "    )\n",
    "\n",
    "scenarios = {\n",
    "    \"pr_a\": pr_score(tests=True, docs=True, small_scope=True, security=True),\n",
    "    \"pr_b\": pr_score(tests=False, docs=True, small_scope=False, security=True),\n",
    "    \"pr_c\": pr_score(tests=True, docs=False, small_scope=True, security=False),\n",
    "}\n",
    "scenarios\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
